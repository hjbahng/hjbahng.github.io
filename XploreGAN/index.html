<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title>Towards Fully Unsupervised Multi-Domain Image-to-Image Translation</title>
    <style type="text/css">
    .center {
        text-align: center;
    }

    .lead {
        font-size: 1.0rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }
    
    .subtitle {
        font-size: 1.6rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }

    .author {
        font-size: 1.1rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }
    
    .title {
        font-size: 2.0rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }

    .figure-caption {
        color: #6c757d;
    }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141263633-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-141263633-1');
    </script>
</head>

<body>
    <div class="container" style="margin-top:20px;">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <center><h1 class="title">Towards Fully Unsupervised <br>Multi-Domain Image-to-Image Translation</h1></center>
                <!-- author -->
                <center><h2 class="author">
                    <a href="https://hjbahng.github.io">Hyojin Bahng</a>&nbsp;&nbsp;&nbsp;
                    <a href="">Sunghyo Chung</a>&nbsp;&nbsp;&nbsp;
                    <a href="https://sjooyoo.github.io">Seungjoo Yoo</a>&nbsp;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>&nbsp;&nbsp;&nbsp;
                </h2></center>
                <center><h2 class="author">
		    Korea University
	        </h2></center>
                <!-- Teaser -->
                <center>
                    <figure class="figure">
                        <img src="./figs/teaser_image_v5.jpg" class="figure-img img-fluid" alt="Responsive image" style="margin:20px 0px;">
                        <figcaption class="figure-caption text-justify">
                            Given raw, unlabeled data, our algorithm discovers multiple attributes that exists in data, and performs high-quality multi-domain image translation in a <span class="font-italic">fully unsupervised</span> setting (i.e., both <span class="font-italic">unpaired</span> and <span class="font-italic">unlabeled</span>). All translation results are based on <span class="font-italic">newly-found</span> attributes from our algorithm (e.g., a wide rage of ethnicity, skin and hair color, age, facial hair, accessories, makeup). We do not utilize any pre-defined attribute labels.
                        </figcaption>
                    </figure>
                </center>
                <!-- Abstract -->
                <hr>
                <div>
                    <h2 class="subtitle">Abstract</h2>
                    <p class="lead text-justify">
                        Despite remarkable success in unpaired image-to-image translation, it has never been "truly unsupervised" (i.e., both <span class="font-italic">unpaired</span> and <span class="font-italic">unlabeled</span>). In other words, images must be labeled according to their domain information. For multi-domain translation tasks, manual annotation becomes highly expensive as a single image has to be assigned multiple domain labels (e.g., 40 attributes for 202,599 images in CelebA dataset). Such requirement for annotated labels fundamentally restricts existing state-of-the-art translation models in its scope of applications. To address this limitation, we propose a fully unsupervised multi-domain image-to-image translation method that can discover multiple unknown domains from unlabeled data and allow translation of fine, disentangled attributes.
                        Experiments show that our method trained on <span class="font-italic">unlabeled</span> data produces high-quality translations, preserves identity, and be perceptually realistic as good as, or better than, state-of-the-art methods trained on <span class="font-italic">labeled</span> data.
                    </p>
                </div>
                <hr>
                <div>
                    <h2 class="subtitle">Paper</h2>
                    <div class="row">
                        <div class="col-sm-3">
                            <img src="./figs/paper.png" class="img-fluid border" alt="" style="margin:20px;">
                        </div>
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <div class="align-middle">
                                <a href="">[Paper]</a>
                                <a href="">[Bibtex]</a>
                                <p class="lead">
                                    Paper <br>
                                    ArXiv, 2019.
                                    <br>
                                    Hyojin Bahng, Sunghyo Chung, Seungjoo Yoo, and Jaegul Choo. "Towards Fully Unsupervised Multi-Domain Image-to-Image Translation".
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                <hr>
                <div>
                    <!-- video -->
                    <h2 class="subtitle">Video</h2>
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/l3XGpWRlU1Q" allowfullscreen></iframe>
                    </div>
                </div>
                <hr>
                <div>
                    <h2 class="subtitle">Method Overview</h2>
                    <div class="row">
                        <div class="col-md-3" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead text-justify align-middle">
                                As illustrated in Fig. 2, XploreGAN is composed of two stages: clustering stage and translation stage. The former discretizes the feature space of images by clustering the high-level features extracted from pre-trained networks. Using the cluster assignment as pseudo-label for each domain, we utilize our newly proposed group instance normalization (GIN) to summarize the common attribute (e.g., blond hair) among images in each cluster and perform high-quality multi-domain translation.
                            </p>
                        </div>
                        <div class="col-md-7">
                            <center>
                                <figure class="figure">
                                    <img src="./figs/model_overview.png" class="figure-img img-fluid" alt="Responsive image" style="margin:0px;">
<!--                                     <figcaption class="figure-caption text-justify">
                                        <center>
                                            Figure 2: Overview of our proposed method.
                                        </center>
                                    </figcaption> -->
                                </figure>
                            </center>
                        </div>
                    </div>
                </div>
                <hr>
                <div>
                    <h2 class="subtitle">Additional Results</h2>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_ffhq.png" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_ffhq2.png" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig4.jpg" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig6.jpg" class="figure-img img-fluid" alt="Responsive image">
                            <!-- <figcaption class="figure-caption text-justify">
                                <center>
                                    Figure 4. Additional qualitative samples of CelebA dataset.
                                </center>
                            </figcaption> -->
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig7.jpg" class="figure-img img-fluid" alt="Responsive image">
                            <!-- <figcaption class="figure-caption text-justify">
                                <center>
                                    Figure 5. Additional qualitative samples of CelebA dataset.
                                </center>
                            </figcaption>
 -->
                        </figure>
                    </center>
                </div>
            </div>
        </div>
    </div>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
</body>

</html>
