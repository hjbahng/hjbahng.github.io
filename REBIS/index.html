<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <title>Learning De-biased Representations with Biased Representations</title>
    <style type="text/css">
    .center {
        text-align: center;
    }

    .lead {
        font-size: 1.0rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }
    
    .subtitle {
        font-size: 1.6rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }

    .author {
        font-size: 1.1rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }
    
    .title {
        font-size: 2.0rem;
        font-family: Georgia, "Times New Roman", Times, serif;
    }
	   
    img.resize {
    max-width:70%;
    max-height:70%;
    }

    .figure-caption {
        color: #6c757d;
	font-family: Georgia, "Times New Roman", Times, serif;
    }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141263633-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-141263633-1');
    </script>
</head>

<body>
    <div class="container" style="margin-top:20px;">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <center><h1 class="title">Learning De-biased Representations<br>with Biased Representations</h1></center>
                <!-- author -->
                <center><h2 class="author">
                    <a href="https://hjbahng.github.io">Hyojin Bahng</a>&nbsp;&nbsp;&nbsp;
                    <a href="">Sanghyuk Chun</a>&nbsp;&nbsp;&nbsp;
                    <a href="">Sangdoo Yun</a>&nbsp;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>&nbsp;&nbsp;&nbsp;
		    <a href="https://seongjoonoh.com/">Seong Joon Oh</a>&nbsp;&nbsp;&nbsp;
                </h2></center>
                <center><h2 class="author">
		    Korea University&nbsp;&nbsp;&nbsp;Clova AI Research 
	        </h2></center>
                <!-- Teaser -->
                <center>
                    <figure class="figure">
                        <img src="./figs/teaser_image.png" class="figure-img img-fluid" alt="Responsive image" style="margin:20px 0px;">
                        <figcaption class="figure-caption text-justify">
				figure caption
                        </figcaption>
                    </figure>
                </center>
                <!-- Abstract -->
                <hr>
                <div>
                    <h2 class="subtitle">Abstract</h2>
                    <p class="lead text-justify">
			    Many machine learning algorithms are trained and evaluated by splitting data from a 
				single source into training and test sets. While such focus on in-distribution learning 
				scenarios has led interesting advances, it has not been able to tell if models are 
				relying on dataset biases as shortcuts for successful prediction (e.g., using snow 
				cues for recognising snowmobiles). Such biased models fail to generalise
				when the bias shifts to a different class. The cross-bias generalisation problem has
				been addressed by de-biasing training data through augmentation or re-sampling,
				which are often prohibitive due to the data collection cost (e.g., collecting images
				of a snowmobile on a desert) and the difficulty of quantifying or expressing biases
				in the first place. In this work, we propose a novel framework to train a de-biased
				representation by encouraging it to be different from a set of representations that
				are biased by design. This tactic is feasible in many scenarios where it is much
				easier to define a set of biased representations than to define and quantify bias. Our
				experiments and analyses show that our method discourages models from taking
				bias shortcuts, resulting in improved performances on de-biased test data.
                    </p>
                </div>
                <hr>
                <div>
                    <h2 class="subtitle">Paper</h2>
                    <div class="row">
                        <div class="col-sm-3">
                            <img src="./figs/paper.png" class="img-fluid border" alt="" style="margin:20px;">
                        </div>
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <div class="align-middle">
                                <a href="https://arxiv.org/abs/1910.02806">[Paper]</a>
                                <p class="lead">
                                    ArXiv, 2019.
                                    <br>
					Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, Seong Joon Oh. "Learning De-biased Representations with Biased Representations".
                                    </p>
                            </div>
                        </div>
                    </div>
                </div>
                <hr>
                <div>
                    <!-- video -->
                    <h2 class="subtitle">Video</h2>
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/l3XGpWRlU1Q" allowfullscreen></iframe>
                    </div>
                </div>
                <hr>
                <div>
                <h2 class="subtitle">Method Overview</h2> 
		    <figure class="figure">
		        <img src="./figs/model_overview.png" class="figure-img img-fluid resize" alt="Responsive image" style="margin:0px 110px;">
		    </figure>
		    <figcaption class="lead text-justify align-middle">
		        XploreGAN is composed of two stages: clustering stage and translation stage. The former discretizes the feature space of images by clustering the high-level features extracted from pre-trained networks. Using the cluster assignment as pseudo-label for each domain, we utilize our newly proposed group instance normalization (GIN) to summarize the common attribute (e.g., blond hair) among images in each cluster and perform high-quality multi-domain translation.
		    </figcaption>
                </div>
                <hr>
                <div>
                    <h2 class="subtitle">Additional Results</h2>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_ffhq.png" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_ffhq2.png" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig4.jpg" class="figure-img img-fluid" alt="Responsive image">
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig6.jpg" class="figure-img img-fluid" alt="Responsive image">
                            <!-- <figcaption class="figure-caption text-justify">
                                <center>
                                    Figure 4. Additional qualitative samples of CelebA dataset.
                                </center>
                            </figcaption> -->
                        </figure>
                    </center>
                    <center>
                        <figure class="figure">
                            <img src="./figs/supp_fig7.jpg" class="figure-img img-fluid" alt="Responsive image">
                            <!-- <figcaption class="figure-caption text-justify">
                                <center>
                                    Figure 5. Additional qualitative samples of CelebA dataset.
                                </center>
                            </figcaption>
 -->
                        </figure>
                    </center>
                </div>
            </div>
        </div>
    </div>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
</body>

</html>
